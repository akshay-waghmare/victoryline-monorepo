# Tasks: 004-scraper-resilience

**Input**: Design documents from `/specs/004-scraper-resilience/` (plan.md, spec.md, research.md, data-model.md, contracts/)  
**Purpose**: Break the plan into concrete, testable tasks organized by user story and phase. Tasks are actionable, include file paths, ownership hints, and acceptance criteria.  
**Profiles**: Tasks support both Standard Profile (8GB+ RAM, 10+ scrapers) and Tiny Profile (4GB RAM, 1-2 scrapers)

## Conventions

- Format: `T### [P?] [US#] Short description (owner) â€” estimate`
  - `[P]` = Can run in parallel
  - `US#` = User Story number from `spec.md` (US1..US5)
  - `[OPS-TINY]` = Specific to Tiny Profile deployment
- Estimates: S/M/L (small/medium/large) â€” approximate engineering points
- Acceptance criteria included on each task line after `--`
- Profile-specific adjustments noted inline (e.g., "standard / tiny")

---

## Phase 1: Setup (Shared Infrastructure)

T001 [P] [FOUNDATION] [X] Create feature branch `004-scraper-resilience` and commit plan artifacts (ops) â€” S
  -- Branch created, plan.md/research.md/data-model.md/quickstart.md/contracts/* present in `specs/004-scraper-resilience/`

T002 [P] [FOUNDATION] [X] Add pinned Playwright and new dependencies to `apps/scraper/crex_scraper_python/requirements.txt` â€” S
  -- Add: `playwright==1.40.0`, `psutil`, `prometheus-client`, `structlog`, `pytest-playwright`; CI installs succeed

T003 [P] [FOUNDATION] [X] Add Dockerfile changes and `docker-compose.prod.yml` resource limits (memory: 2560M standard / 800M tiny) for scraper service â€” S
  -- Container builds; `docker run` respects memory limit; compose passes `SCRAPER_MAX_LIFETIME_HOURS` env; docker-compose.tiny.yml override created

T004 [P] [FOUNDATION] [X] Configure linting and formatting for Python (black/isort/flake8) in `apps/scraper/` â€” S
  -- Pre-commit hook added; `make lint` passes

---

## Phase 2: Foundational (Blocking Prerequisites)

T005 [FOUNDATION] [X] Implement environment configuration loader and defaults in `apps/scraper/crex_scraper_python/config.py` â€” M
  -- All env vars from plan.md are supported with defaults; integration tests can override

T006 [FOUNDATION] [X] Implement `ScraperContext` skeleton in `apps/scraper/crex_scraper_python/scraper_context.py` â€” M
  -- Tracks start_time, last_update, memory, error_count; exposes `should_restart()` per spec

T007 [FOUNDATION] [X] Implement `retry_utils.py` with exponential backoff + jitter in `apps/scraper/crex_scraper_python/retry_utils.py` â€” M
  -- Retry decorator available; unit tests for timing and max attempts

T008 [FOUNDATION] [X] Add `circuit_breaker.py` implementation and unit tests in `apps/scraper/crex_scraper_python/circuit_breaker.py` â€” M
  -- States CLOSED/OPEN/HALF_OPEN; configurable thresholds; tests for transitions

T009 [FOUNDATION] [X] Implement `db_pool.py` SQLite connection pool in `apps/scraper/crex_scraper_python/db_pool.py` â€” M
  -- Thread-safe pool; contextmanager `get_connection()`; tests for concurrent usage

T010 [FOUNDATION] [X] Implement `BatchWriter` for batched DB writes in `apps/scraper/crex_scraper_python/persistence/batch_writer.py` â€” M
  -- Batch size default 20 (standard profile) / 15 (tiny profile); critical events bypass batching; flush interval 5s; metrics emitted

T011 [FOUNDATION] [X] Add health endpoint scaffold to `apps/scraper/crex_main_url.py` returning `HealthResponse` per `contracts/health-api.yaml` â€” M
  -- GET /health returns fields and responds <100ms in local tests

T012 [FOUNDATION] [X] Add Prometheus metrics exposition in `monitoring.py` and start HTTP metrics server on port 9090 â€” M
  -- Metrics: scraper_errors_total, scraper_updates_total, scraper_update_latency_seconds, scraper_memory_bytes, active_scrapers_count, data_staleness_seconds

T013 [FOUNDATION] [X] Structured logging with `structlog` in `apps/scraper/crex_scraper_python/logging_config.py` â€” S
  -- JSON logs with timestamp, severity, match_id, scraper_id present

T014 [FOUNDATION] [X] Add graceful shutdown (SIGTERM/SIGINT) handling in `run_server.py` and `ScraperContext._cleanup()` â€” M
  -- Container shutdown completes cleanup within 30s, tests simulate SIGTERM

T015 [FOUNDATION] [X] Add unit tests baseline and CI pipeline step to run `pytest` for scraper (CI config) â€” M
  -- CI job runs `pytest -q`; coverage goal target tracked

---

## US1: Continuous Live Data Availability (Priority: P1) ðŸŽ¯

Goal: Ensure live match data is continuously available and recovers automatically from transient issues.

T101 [US1] [X] Add selector fallback arrays and parsing utility in `apps/scraper/crex_scraper_python/parsers.py` â€” M
  -- SELECTORS dict defined; parser tries fallback array and logs which selector succeeded; monitored by alert rule

T102 [US1] [X] Integrate Playwright lifecycle with context managers in `crex_scraper.py` (open/close contexts/pages properly) â€” M
  -- No orphaned Chromium processes after successful run; integration test asserts process count drops

T103 [US1] [X] Implement network retry flows using `retry_utils` around network calls and Playwright navigation â€” M
  -- Retries with delays [1,2,4,8,16] and jitter; metrics incremented on each retry attempt

T104 [US1] [X] Implement soft-memory detection and graceful restart in `ScraperContext` (trigger at 1.5GB standard / 650MB tiny) â€” M
  -- When memory threshold met: complete cycle, save state snapshot, restart within 60s; 14 unit tests passing + smoke test; full acceptance deferred to US1 completion

T105 [US1] [X] Implement state snapshot save/load for seamless restart in `scraper_state.py` â€” M
  -- ScraperStateSnapshot dataclass + StateStore (SQLite); ScraperContext.create_state_snapshot() method; 10 unit tests passing (0.39s); resume logic deferred to actual scraper integration

T106 [US1] Add acceptance tests (pytest + playwright) `tests/acceptance/test_automatic_recovery_from_network_failure.py` â€” L
  -- Simulates network interruption and validates recovery <30s with zero critical data loss

---

## US2: Automatic Failure Recovery (Priority: P1) ðŸŽ¯

Goal: Detect and recover from failures with minimal human intervention.

T201 [US2] Wire up `CircuitBreaker` to protected operations in `crex_scraper.py` and `cricket_data_service.py` â€” M
  -- Circuit opens after 5 consecutive failures, HALF_OPEN probe logic implemented

T202 [US2] Implement orphaned Chromium process scanner & terminator in `monitoring/cleanup_orphans.py` (runs every 30m standard / 60m tiny) â€” M
  -- Orphans terminated, actions logged with PID and memory

T203 [US2] Implement token refresh flow in `auth.py` that calls backend `/token/generate-token` on 401/403 â€” M
  -- Token refresh retried with exponential backoff; original request retried once after refresh

T204 [US2] Add memory / lifetime auto-restart orchestration (6h max lifetime) with coordinated handoff â€” M
  -- Restart occurs at low-activity period; state persisted and reload validated

T205 [US2] Add unit and integration tests for circuit breaker behavior and restart flows â€” M
  -- Tests cover open/close/half-open transitions and restart acceptance criteria

---

## US3: Health Monitoring and Observability (Priority: P2)

Goal: Provide real-time visibility and alerting for scraper health and data freshness.

T301 [US3] Implement `/metrics` exposition (Prometheus) using `prometheus_client` in `monitoring.py` â€” M
  -- Metrics available on :9090 and scrapeable by Prometheus; unit test verifies metrics endpoint

T302 [US3] Create Prometheus scrape config `monitoring/prometheus.yml` and docker-compose monitoring stack (48h retention for tiny profile) â€” S
  -- Prometheus scrapes scraper:9090 and backend:8080; local compose brings up monitoring stack (Grafana optional for tiny); scrape interval 15s standard / 30s tiny

T303 [US3] Implement Grafana dashboards (`monitoring/grafana/dashboards/`) including RSS per-scraper and data freshness (optional for tiny profile) â€” M
  -- Dashboards imported and render metrics from local Prometheus instance; Prometheus UI acceptable alternative for tiny profile

T304 [US3] Implement Alertmanager rules and inhibit rules in `monitoring/alertmanager.yml` grouped by `match_id` â€” S
  -- Alerts group by match_id; inhibit rules configured to silence warnings when critical present

T305 [US3] Add Prometheus alerting rules (data stale 5m, error rate >10/min standard or >6/min tiny, memory >80% soft limit) in `monitoring/prometheus.rules.yml` â€” M
  -- Alerts fire in simulated degraded conditions during integration tests; thresholds adjusted per deployment profile

T306 [US3] Implement clock skew metric `clock_skew_seconds` and require NTP in quickstart and infra docs â€” S
  -- Metric available, test verifies skew <500ms in staging

---

## US4: Resource Management and Efficiency (Priority: P2)

Goal: Efficient resource usage enabling 10+ concurrent scrapers (standard) or 1-2 scrapers (tiny profile).

T401 [US4] Integrate `db_pool.py` and `BatchWriter` into persistence flow (`persistence/*`) â€” M
  -- Batch writes operate correctly; critical events persisted immediately

T402 [US4] Configure HikariCP properties in `apps/backend/spring-security-jwt/src/main/resources/application.properties` and tests â€” S
  -- Connection pool metrics exposed; backend load tests show stable connection wait times <100ms

T403 [US4] Implement per-scraper memory monitoring using `psutil` and export `scraper_memory_bytes` gauge â€” M
  -- RSS recorded per match_id; Grafana visualizations show per-scraper memory time series

T404 [US4] Add lifecycle watchdog that enforces Docker memory policy and reports when host low memory prevents new scrapers from starting â€” S
  -- Host refuses new scraper starts when available memory <500MB and queues start request

T405 [US4] Load test: 10 concurrent scrapers for 12 hours (standard) or 2 scrapers for 6-8 hours (tiny) in staging and record KPIs â€” L
  -- Standard: Memory per-scraper <1GB avg, total <10GB, no monotonic growth; Tiny: Memory per-scraper <700MB avg, total <2GB, system free RAM >1GB; runbook for failures documented

---

## US5: Adaptive Performance Under Load (Priority: P3)

Goal: Adjust scraper behavior dynamically based on observed conditions.

T501 [US5] Implement `AdaptivePoller` in `crex_scraper_python/adaptive.py` (adjust intervals and timeouts per profile) â€” M
  -- Behavior matches acceptance scenarios: Standard (2s/2.5s/5s/10s/20s/30s) or Tiny (3s/5s/10s/20s/30s) according to recent error/change rates

T502 [US5] Add logic to increase timeouts by 50% under high-latency observations and decrease when stable â€” M
  -- Page timeout adapts from 30s â†’ 45s and back; tests simulate latency profiles

T503 [US5] Implement priority-based allocation for high-priority matches (increase poll frequency) â€” M
  -- High-priority matches get more resources and higher polling frequency; documented config

---

## Cross-Cutting & Operational Tasks

T701 [P] [OPS] Pin Playwright version in `requirements.txt` and add upgrade playbook `RELEASE_PLAYWRIGHT.md` â€” S
  -- Playwright pinned; playbook documents how to test DOM timing regressions

T702 [P] [OPS] Add Grafana RSS dashboard (memory-per-scraper.json) to `monitoring/grafana/dashboards/` â€” S
  -- Dashboard JSON present; validated in staging

T703 [P] [OPS] Alertmanager inhibit and group rules implemented (see plan operational safeguards) â€” S
  -- Inhibit rules tested; alert noise reduced in simulated storm

T704 [P] [OPS] Configure NTP on hosts and document in `quickstart.md` and infra README â€” S
  -- NTP configured; clock_skew_seconds metric <500ms in staging

T705 [P] [OPS] Update CI/CD pipeline to run acceptance tests in staging on each PR to `004-scraper-resilience` â€” M
  -- PR pipeline runs acceptance suite; failures block merges

T706 [P] [OPS] Create runbook for OOM conditions and automatic remediation steps (restart sequence, alert, rollback) â€” S
  -- Runbook checked into `docs/ops/runbooks/scraper-oom.md`

T707 [P] [OPS-TINY] Add Chromium optimization flags to Playwright launch config in `run_server.py` â€” S
  -- Flags added: --disable-dev-shm-usage, --disable-gpu, --no-sandbox, --disable-extensions, --mute-audio, --single-process; memory savings validated (200-300MB)

T708 [P] [OPS-TINY] Create docker-compose.tiny.yml override file with 4GB RAM VPS configuration â€” S
  -- Override file created with 800M memory limits, adjusted env vars, Chromium flags; docker-compose -f docker-compose.yml -f docker-compose.tiny.yml validates

T709 [P] [OPS-TINY] Document tiny profile deployment in `quickstart.md` with RAM constraints and setup instructions â€” M
  -- Tiny profile section added with resource allocation strategy, phased deployment plan, trade-offs documented

T710 [P] [OPS-TINY] Set up zram swap on VPS host (optional but recommended for tiny profile) â€” S
  -- zram configured with 2GB compressed swap; documented in infra setup guide

T711 [P] [OPS-TINY] Create monitoring runbook for tiny profile (free RAM checks, OOM handling, single-scraper recovery) â€” M
  -- Runbook covers: memory pressure detection, graceful degradation to 1 scraper, resource monitoring commands

---

## Polish & Release

T901 [P] Documentation: finalize `quickstart.md`, `README.md`, and `IMPLEMENTATION_SUMMARY.md` â€” S
  -- Docs updated and validated by a peer

T902 [P] Release: Deploy monitoring stack to staging, run Phase 2 load tests, iterate configs â€” L
  -- Monitoring stack live, dashboards show metrics, load tests pass KPIs

T903 [P] Final Acceptance: Run full acceptance suite and get signoff from Product/Ops â€” M
  -- 100% acceptance tests pass; signoff recorded in feature ticket

---

## Estimates & Priorities (summary)

- Critical path (Phase 2 Foundational + US1 + US2): ~3-4 weeks (with 2 engineers)  
- Full feature rollout with optimizations (Phases 3-6): ~8 weeks total as described in plan.md
- **Tiny Profile**: Skip or defer Phase 6 optimization tasks; focus on Phases 1-5 (~6 weeks)

## Deployment Profile Selection

**Standard Profile (8GB+ RAM)**:
- All tasks apply as written
- Target: 10+ concurrent scrapers
- Full monitoring stack: Prometheus + Grafana + Alertmanager

**Tiny Profile (4GB RAM VPS)**:
- Additional tasks: T707-T711 (Chromium optimization, docker-compose.tiny.yml, zram, docs)
- Modified tasks: T003, T010, T104, T202, T302, T303, T305, T405, T501 (see adjustments above)
- Target: 1-2 concurrent scrapers maximum
- Lightweight monitoring: Prometheus + Alertmanager (Grafana optional)
- Phased deployment: 1 scraper (Week 1) â†’ 2 scrapers if stable (Week 2) â†’ monitoring (Week 3)

## How to use

**Standard Profile**:
1. Start with Foundational tasks (T005..T015). These block all user story work.  
2. Parallelize US1 and US2 tasks after foundational completion.  
3. Run T705 to wire acceptance tests into CI and gate merges.  
4. Use T401/T405 load tests and dashboards (T702) to tune memory/disk/container limits.

**Tiny Profile**:
1. Complete T001-T004 with docker-compose.tiny.yml (T708)
2. Add Chromium optimization flags (T707) before Foundational tasks
3. Complete Foundational tasks (T005..T015) with tiny profile batch sizes
4. Implement US1 and US2 with adjusted memory thresholds
5. Deploy single scraper first (Week 1), validate 24h stability before scaling to 2
6. Monitor system free RAM; if consistently <1GB, remain at 1 scraper
7. Skip or defer US5 (Adaptive Performance) and Phase 6 (Optimization) initially

---

## Generated by `/speckit.tasks` on 2025-11-12
# Tasks: Scraper Resilience Implementation

**Feature**: 004-scraper-resilience  
**Created**: 2025-11-12  
**Status**: Not Started

## Task Breakdown by Phase

### Phase 1: Foundation and Resource Management

#### Task 1.1: Create ScraperContext Module
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Create `apps/scraper/crex_scraper_python/scraper_context.py` with lifecycle management
- **Acceptance Criteria**:
  - ScraperContext class tracks start_time, last_update, error_count, memory_usage
  - Implements `should_restart()` logic for age, staleness, errors
  - Provides `health_status` property (healthy/degraded/failing)
  - Includes graceful shutdown handling
- **Dependencies**: None
- **Testing**: Unit tests for state transitions and thresholds

#### Task 1.2: Implement Browser Resource Cleanup
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Refactor `fetchData()` in crex_scraper.py for guaranteed cleanup
- **Acceptance Criteria**:
  - Wrap browser lifecycle in try/finally blocks
  - Close all contexts, pages, browser on exit
  - Track browser PIDs for orphan detection
  - Verify cleanup with process monitoring
- **Dependencies**: Task 1.3 (process monitoring)
- **Testing**: Integration tests with forced failures

#### Task 1.3: Add Process Monitoring Utilities
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Create process monitoring module using psutil
- **Acceptance Criteria**:
  - Function to get memory usage (RSS) for process
  - Function to list child processes (browser instances)
  - Function to terminate process tree
  - Function to detect orphaned processes
- **Dependencies**: Install psutil library
- **Testing**: Unit tests for various process scenarios

#### Task 1.4: Implement Health Check Endpoint
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Add /health endpoint to crex_main_url.py
- **Acceptance Criteria**:
  - Returns JSON with overall status, active scraper count
  - Includes per-match metrics: last_update, error_count, memory_mb
  - Responds within 100ms
  - Used by Docker healthcheck
- **Dependencies**: Task 1.1 (ScraperContext for metrics)
- **Testing**: Integration tests for various health states

#### Task 1.5: Add Structured Logging
- **Priority**: P2
- **Estimate**: 1 day
- **Assignee**: TBD
- **Description**: Implement structured logging with severity levels
- **Acceptance Criteria**:
  - Use structlog library for JSON logging
  - Severity levels: DEBUG, INFO, WARN, ERROR, CRITICAL
  - Include context: timestamp, match_id, scraper_id, message
  - Log rotation configured
- **Dependencies**: Install structlog library
- **Testing**: Verify log output format and levels

#### Task 1.6: Update Docker Configuration
- **Priority**: P2
- **Estimate**: 1 day
- **Assignee**: TBD
- **Description**: Add resource limits to docker-compose.prod.yml
- **Acceptance Criteria**:
  - Memory limits: 2G hard, 1G reservation
  - CPU limits: 2 CPUs max
  - Environment variables for configuration
  - Healthcheck uses /health endpoint
- **Dependencies**: Task 1.4 (health endpoint)
- **Testing**: Deploy to staging, verify limits enforced

#### Task 1.7: Run 24-Hour Memory Leak Test
- **Priority**: P1
- **Estimate**: 1 day (setup + monitoring)
- **Assignee**: TBD
- **Description**: Validate no memory leaks over 24 hours in staging
- **Acceptance Criteria**:
  - Memory usage stable (Â±10% variance)
  - No browser process accumulation
  - All cleanup verified
  - Document baseline metrics
- **Dependencies**: All Phase 1 tasks complete
- **Testing**: Automated memory monitoring script

---

### Phase 2: Resilience Patterns

#### Task 2.1: Create Circuit Breaker Module
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Implement `apps/scraper/crex_scraper_python/circuit_breaker.py`
- **Acceptance Criteria**:
  - States: CLOSED, OPEN, HALF_OPEN with transitions
  - Configurable failure threshold (default 5)
  - Configurable timeout (default 60s)
  - call() wrapper for protected operations
  - Thread-safe implementation
- **Dependencies**: None
- **Testing**: Unit tests for all state transitions

#### Task 2.2: Implement Retry Utilities
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Create `apps/scraper/crex_scraper_python/retry_utils.py`
- **Acceptance Criteria**:
  - retry_with_backoff() function
  - Exponential delays: 1s, 2s, 4s, 8s, 16s
  - Configurable max_retries and delays
  - Detailed logging of retry attempts
  - Decorator pattern support
- **Dependencies**: None
- **Testing**: Unit tests with mock failures

#### Task 2.3: Integrate Circuit Breaker with Scraping
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Add circuit breaker to critical scraping operations
- **Acceptance Criteria**:
  - Wrap page navigation with circuit breaker
  - Wrap API calls with circuit breaker
  - Handle CircuitBreakerOpen exceptions gracefully
  - Log circuit breaker state changes
- **Dependencies**: Task 2.1
- **Testing**: Integration tests with induced failures

#### Task 2.4: Add Retry Logic to Network Requests
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Apply retry logic to all network operations
- **Acceptance Criteria**:
  - Retry HTTP requests to backend API
  - Retry page navigation on timeout
  - Retry DOM element queries
  - Proper logging of retries
- **Dependencies**: Task 2.2
- **Testing**: Integration tests with network failures

#### Task 2.5: Implement Token Refresh Handling
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Handle JWT token expiration automatically
- **Acceptance Criteria**:
  - Detect 401/403 responses
  - Request fresh token from /token/generate-token
  - Update internal auth state
  - Retry original failed request
  - Cache valid tokens with expiration tracking
- **Dependencies**: Task 2.2 (retry logic)
- **Testing**: Integration tests with expired tokens

#### Task 2.6: Create Database Connection Pool
- **Priority**: P2
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Implement `apps/scraper/crex_scraper_python/db_pool.py`
- **Acceptance Criteria**:
  - SQLite connection pool (5-10 connections)
  - Thread-safe get_connection() context manager
  - Connection health checks
  - Automatic connection recycling
  - Connection leak detection
- **Dependencies**: None
- **Testing**: Unit tests with concurrent access

#### Task 2.7: Implement Graceful Shutdown
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Add signal handling for graceful shutdown
- **Acceptance Criteria**:
  - Handle SIGTERM, SIGINT signals
  - Complete current scraping cycle
  - Close all browser instances
  - Flush in-memory queues
  - Save scraper state
  - Terminate within 30 seconds
- **Dependencies**: Task 1.1, 1.2
- **Testing**: Integration tests with signal sending

#### Task 2.8: Add Environment Variable Configuration
- **Priority**: P2
- **Estimate**: 1 day
- **Assignee**: TBD
- **Description**: Externalize all configuration to environment variables
- **Acceptance Criteria**:
  - Create config module with defaults
  - Validate configuration at startup
  - Document all environment variables
  - Provide example .env file
- **Dependencies**: All Phase 2 tasks
- **Testing**: Verify configuration loading

---

### Phase 3: Monitoring and Observability

#### Task 3.1: Install Prometheus Client
- **Priority**: P1
- **Estimate**: 0.5 days
- **Assignee**: TBD
- **Description**: Add prometheus_client to requirements.txt
- **Acceptance Criteria**:
  - Install prometheus_client==0.19.0
  - Update requirements.txt
  - Verify import in Python
- **Dependencies**: None
- **Testing**: Import verification

#### Task 3.2: Create Monitoring Module
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Implement `apps/scraper/crex_scraper_python/monitoring.py`
- **Acceptance Criteria**:
  - Define Prometheus metrics: Counter, Gauge, Histogram
  - Expose metrics on port 9090
  - Metrics: errors_total, updates_total, update_latency, memory_bytes, active_scrapers
  - Label metrics by match_id, error_type
  - start_metrics_server() function
- **Dependencies**: Task 3.1
- **Testing**: Verify metrics endpoint responds

#### Task 3.3: Add Metrics Collection Points
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Instrument scraper code with metrics
- **Acceptance Criteria**:
  - Increment error counters on failures
  - Increment update counters on success
  - Track update latency with histogram
  - Track memory usage with gauge
  - Update active scraper count
- **Dependencies**: Task 3.2
- **Testing**: Verify metrics update correctly

#### Task 3.4: Implement Staleness Detection
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Add background task for staleness monitoring
- **Acceptance Criteria**:
  - Check last_update_time every 60 seconds
  - Detect staleness >5 minutes during active match
  - Log WARNING with context
  - Trigger auto-restart sequence
  - Update health status to degraded
- **Dependencies**: Task 1.1, 1.4
- **Testing**: Integration tests with stale data

#### Task 3.5: Add Data Freshness Headers (Backend)
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Update backend API to include freshness metadata
- **Acceptance Criteria**:
  - Add X-Data-Freshness header (ISO 8601 timestamp)
  - Add X-Data-Age-Seconds header
  - Update all match data endpoints
  - Document headers in API docs
- **Dependencies**: Backend API access
- **Testing**: API integration tests

#### Task 3.6: Create Grafana Dashboards
- **Priority**: P2
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Design and implement Grafana dashboards
- **Acceptance Criteria**:
  - Scraper Health Overview dashboard
  - Resource Usage dashboard
  - Data Freshness dashboard
  - Performance Metrics dashboard
  - Export dashboard JSON templates
- **Dependencies**: Task 3.2, 3.3
- **Testing**: Visual validation of dashboards

#### Task 3.7: Configure Prometheus Scraping
- **Priority**: P2
- **Estimate**: 1 day
- **Assignee**: TBD
- **Description**: Set up Prometheus configuration
- **Acceptance Criteria**:
  - Create prometheus.yml configuration
  - Add scraper targets
  - Configure scrape interval (15s)
  - Set up retention policy
- **Dependencies**: Task 3.2
- **Testing**: Verify Prometheus targets healthy

#### Task 3.8: Document Monitoring Setup
- **Priority**: P3
- **Estimate**: 1 day
- **Assignee**: TBD
- **Description**: Create monitoring documentation
- **Acceptance Criteria**:
  - Setup instructions for Prometheus/Grafana
  - Metrics glossary
  - Dashboard usage guide
  - Alert configuration examples
  - Troubleshooting guide
- **Dependencies**: All Phase 3 tasks
- **Testing**: Follow documentation for setup

---

### Phase 4: Adaptive Behavior

#### Task 4.1: Implement Adaptive Polling Intervals
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Add dynamic polling interval calculation
- **Acceptance Criteria**:
  - Baseline 2.5 seconds during active play
  - Increase to 5s/10s/20s on errors
  - Reduce to 30s during detected breaks
  - Optimize to 2s during high-frequency updates
  - Configurable thresholds
- **Dependencies**: Task 1.1 (ScraperContext)
- **Testing**: Unit tests for various scenarios

#### Task 4.2: Add Match State Detection
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Detect match state from update patterns
- **Acceptance Criteria**:
  - Detect active play vs breaks
  - Detect innings changes
  - Detect match end
  - State machine for transitions
  - Log state changes
- **Dependencies**: None
- **Testing**: Integration tests with various matches

#### Task 4.3: Implement Adaptive Timeouts
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Adjust timeouts based on network latency
- **Acceptance Criteria**:
  - Measure average latency over last 10 requests
  - Increase timeouts by 50% if latency >2s
  - Decrease timeouts by 20% if latency <500ms
  - Minimum timeout 15 seconds
  - Log timeout adjustments
- **Dependencies**: None
- **Testing**: Unit tests with simulated latency

#### Task 4.4: Add Error-Based Backoff
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Implement progressive backoff on errors
- **Acceptance Criteria**:
  - Increase polling interval on consecutive errors
  - Reset interval after successful updates
  - Configurable error thresholds
  - Log backoff decisions
- **Dependencies**: Task 4.1
- **Testing**: Integration tests with induced errors

#### Task 4.5: Implement Priority-Based Allocation
- **Priority**: P3
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Prioritize resources for important matches
- **Acceptance Criteria**:
  - Define priority levels (high/medium/low)
  - Allocate more resources to high-priority
  - Adjust polling based on priority
  - Configure priority per match
- **Dependencies**: Task 1.1
- **Testing**: Integration tests with mixed priorities

#### Task 4.6: Test Adaptive Behavior
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Comprehensive testing of adaptive systems
- **Acceptance Criteria**:
  - Test under various network conditions
  - Test with different match states
  - Test error recovery scenarios
  - Validate resource optimization
  - Document adaptive behavior
- **Dependencies**: All Phase 4 tasks
- **Testing**: Simulation tests

---

### Phase 5: Production Deployment

#### Task 5.1: Prepare Production Configuration
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Set up production environment variables
- **Acceptance Criteria**:
  - Create production .env file
  - Set appropriate thresholds
  - Configure monitoring endpoints
  - Set up secret management
  - Validate configuration
- **Dependencies**: All previous phases
- **Testing**: Configuration validation

#### Task 5.2: Update Docker Compose for Production
- **Priority**: P1
- **Estimate**: 1 day
- **Assignee**: TBD
- **Description**: Enhance docker-compose.prod.yml
- **Acceptance Criteria**:
  - Add resource limits
  - Configure environment variables
  - Set up volumes for persistence
  - Add monitoring containers
  - Document deployment
- **Dependencies**: Task 5.1
- **Testing**: Staging deployment validation

#### Task 5.3: Canary Deployment (Single Scraper)
- **Priority**: P1
- **Estimate**: 2 days (includes monitoring)
- **Assignee**: TBD
- **Description**: Deploy single scraper to production
- **Acceptance Criteria**:
  - Deploy one scraper instance
  - Monitor for 24 hours
  - Validate all metrics
  - Check for errors/issues
  - Document observations
- **Dependencies**: Task 5.2
- **Testing**: Production validation

#### Task 5.4: Scale to 3 Scrapers
- **Priority**: P1
- **Estimate**: 3 days (includes monitoring)
- **Assignee**: TBD
- **Description**: Expand to 3 concurrent scrapers
- **Acceptance Criteria**:
  - Deploy 3 scraper instances
  - Monitor for 48 hours
  - Validate resource usage
  - Check for interference
  - Tune configuration if needed
- **Dependencies**: Task 5.3
- **Testing**: Multi-scraper validation

#### Task 5.5: Full Production Rollout
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Deploy to all active matches
- **Acceptance Criteria**:
  - Deploy all required scrapers
  - Monitor for 1 week
  - Achieve 99.5% uptime
  - Data freshness <10s
  - Zero manual interventions
- **Dependencies**: Task 5.4
- **Testing**: Full production validation

#### Task 5.6: Create Operational Runbook
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Document operational procedures
- **Acceptance Criteria**:
  - Troubleshooting guide for common issues
  - Manual restart procedures
  - Configuration change procedures
  - Monitoring and alerting guide
  - Escalation procedures
- **Dependencies**: All previous tasks
- **Testing**: Runbook validation

#### Task 5.7: Conduct Team Training
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Train operations team on new system
- **Acceptance Criteria**:
  - Runbook walkthrough
  - Hands-on troubleshooting practice
  - Dashboard usage training
  - Q&A session
  - Feedback collection
- **Dependencies**: Task 5.6
- **Testing**: Simulated incident response

---

### Phase 6: Optimization and Scaling

#### Task 6.1: Analyze Production Metrics
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Identify bottlenecks and optimization opportunities
- **Acceptance Criteria**:
  - Collect 2+ weeks of metrics
  - Identify slow queries
  - Find memory hotspots
  - Analyze error patterns
  - Create optimization report
- **Dependencies**: Task 5.5 (production data)
- **Testing**: Data analysis validation

#### Task 6.2: Optimize Database Queries
- **Priority**: P2
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Add indexes and optimize SQL
- **Acceptance Criteria**:
  - Analyze slow queries
  - Add appropriate indexes
  - Optimize JOIN operations
  - Test query performance
  - Document changes
- **Dependencies**: Task 6.1
- **Testing**: Performance benchmarks

#### Task 6.3: Implement Redis Caching (Optional)
- **Priority**: P3
- **Estimate**: 5 days
- **Assignee**: TBD
- **Description**: Add Redis layer for frequently accessed data
- **Acceptance Criteria**:
  - Set up Redis container
  - Implement caching layer
  - Configure cache TTLs
  - Add cache invalidation
  - Measure performance improvement
- **Dependencies**: Redis infrastructure
- **Testing**: Cache hit rate validation

#### Task 6.4: Optimize Browser Configuration
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Tune Playwright/browser settings for performance
- **Acceptance Criteria**:
  - Disable unnecessary features
  - Optimize resource loading
  - Tune memory settings
  - Test performance impact
  - Document optimal settings
- **Dependencies**: None
- **Testing**: Performance benchmarks

#### Task 6.5: Test Scaling to 20+ Matches
- **Priority**: P1
- **Estimate**: 3 days
- **Assignee**: TBD
- **Description**: Validate support for 20+ concurrent scrapers
- **Acceptance Criteria**:
  - Deploy 20 scraper instances
  - Monitor for 24 hours
  - Validate reliability metrics
  - Check resource usage
  - Identify scaling limits
- **Dependencies**: All optimization tasks
- **Testing**: Load testing

#### Task 6.6: Implement Horizontal Scaling
- **Priority**: P2
- **Estimate**: 4 days
- **Assignee**: TBD
- **Description**: Enable multiple scraper containers
- **Acceptance Criteria**:
  - Distribute scrapers across containers
  - Implement work distribution
  - Add container orchestration
  - Test multi-container deployment
  - Document scaling procedures
- **Dependencies**: Task 6.5
- **Testing**: Multi-container validation

#### Task 6.7: Load Test at Tournament Scale
- **Priority**: P1
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Simulate peak tournament load
- **Acceptance Criteria**:
  - Simulate 30+ concurrent matches
  - Test for 4+ hours
  - Validate reliability metrics maintained
  - Identify breaking points
  - Document capacity limits
- **Dependencies**: Task 6.6
- **Testing**: Load testing with locust

#### Task 6.8: Document Scaling Procedures
- **Priority**: P2
- **Estimate**: 2 days
- **Assignee**: TBD
- **Description**: Create scaling playbook
- **Acceptance Criteria**:
  - Horizontal scaling procedures
  - Capacity planning guidelines
  - Performance tuning guide
  - Troubleshooting scaling issues
  - Update operational runbook
- **Dependencies**: All Phase 6 tasks
- **Testing**: Documentation validation

---

## Summary Statistics

**Total Tasks**: 50+  
**Estimated Duration**: 8 weeks  
**Priority Breakdown**:
- P1 (Critical): 28 tasks
- P2 (High): 18 tasks
- P3 (Medium): 4 tasks

**Phase Duration Estimates**:
- Phase 1: 2 weeks
- Phase 2: 2 weeks
- Phase 3: 1 week
- Phase 4: 1 week
- Phase 5: 1 week
- Phase 6: 1+ weeks

---

**Document Version**: 1.0  
**Last Updated**: 2025-11-12  
**Owner**: Scraper Resilience Team
